{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26d109f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/dtypes.py:585: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  np.object,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32988/63054873.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimport_data_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcutims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoxels_ROIs_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_data_for_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnet_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mor/wisdrc/experimental/fmri_compare/import_data_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspearmanr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkendalltau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Bring in subpackages.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# from tensorflow.python import keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdense_to_ragged_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdense_to_sparse_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/experimental/service/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_dataset_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/experimental/ops/data_service_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompression_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_options\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoShardPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_options\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExternalStatePolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/experimental/ops/compression_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_experimental_dataset_ops\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mged_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwrapt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomposite_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/util/nest.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_six\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse_tensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sparse_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/sparse_tensor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomposite_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mop_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0;31m# strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mtypes_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDT_STRING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m     \u001b[0mtypes_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDT_COMPLEX64\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m__former_attrs__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__former_attrs__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;31m# Importing Tester requires importing all of UnitTest which is not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "from import_data_utils import *\n",
    "sub = 2\n",
    "cutims, voxels_ROIs_full = import_data_for_sub(sub, path)\n",
    "\n",
    "net_size = 224\n",
    "batch_size = 64\n",
    "net_lim = (len(cutims) // batch_size) * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f202ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECoG\n",
    "with open('/home/mor/ECoG/subjects_data/RDMv_zscore_dist10.pkl', 'rb') as f:\n",
    "    RDMv= pkl.load(f)\n",
    "\n",
    "with open('/home/mor/ECoG/subjects_data/all_images.pkl', 'rb') as f:\n",
    "    cutims= pkl.load(f)\n",
    "\n",
    "# RDMv, all_images\n",
    "cutims = cv2.normalize(cutims, None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_32F)\n",
    "cutims = cutims.astype(np.uint8)\n",
    "\n",
    "# Ecog\n",
    "batch_size = 50\n",
    "net_lim = (len(cutims) // batch_size) * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e76a852",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_n = 1\n",
    "# allocating memory and creating the Keras net\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(physical_devices[gpu_n], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[gpu_n], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32891faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image resizing and padding\n",
    "def resize_padding(x, des_size):\n",
    "    \"\"\"\n",
    "    This function takes the image which is already in the desired size, but minimizes it to a smaller size\n",
    "    and add padding to keep the original size.  \n",
    "    \"\"\"\n",
    "    if x.shape[0] < des_size:\n",
    "        raise ValueError(\"Desired size must be smaller than original size!\")\n",
    "    neutral_color = x[0,0,0]  #pixel from the image surrounding\n",
    "    padding = np.ones(x.shape) * neutral_color\n",
    "    start =(x.shape[0] - des_size)//2\n",
    "    end = start + des_size\n",
    "    resized_im = keras.layers.Resizing(des_size, des_size, interpolation='bilinear', crop_to_aspect_ratio=False,)(x).numpy()\n",
    "    padding[start:end, start:end] = resized_im\n",
    "    return padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86e23de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zoomin \n",
    "def zoomin_img(img, zoomsize):\n",
    "    \"\"\"\n",
    "    This function will zoom in the image by increasing it to a bigger size and then cut and use only the middle square of it \n",
    "    to enter the network.\n",
    "    \n",
    "    img: an image which is already the desired size!\n",
    "    zoomin = the new bigger size which we will zoom into\n",
    "    \"\"\"\n",
    "    origin_sz = img.shape[0]\n",
    "    cut = (zoomsize-origin_sz)//2\n",
    "    newim = keras.layers.Resizing(zommin, zommin, interpolation='bilinear', crop_to_aspect_ratio=False,)(img).numpy()\n",
    "    cutnewim = newim[cut:origin_sz+cut,cut:origin_sz+cut]\n",
    "    return cutnewim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a55eb7",
   "metadata": {},
   "source": [
    "### examples of pictures filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ede1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,4, figsize=(16, 8))\n",
    "im1_sub1= cutims[3]\n",
    "axs[0].imshow(im1_sub1, vmin=0, vmax=255)\n",
    "axs[0].set_title(\"Original presented image\")\n",
    "\n",
    "#example of image cutting\n",
    "# axs[1].imshow(im1_sub1, vmin=0, vmax=255)\n",
    "\n",
    "#example of image blurring\n",
    "im1_blur = cv2.blur(im1_sub1,(7,7))\n",
    "axs[1].imshow(im1_blur.astype(int))\n",
    "axs[1].set_title(\"Blurred image\")\n",
    "\n",
    "# example of the resizing the picture and adding paddings.\n",
    "img = keras.layers.Resizing(224, 224, interpolation='bilinear', crop_to_aspect_ratio=False,)(im1_sub1).numpy()\n",
    "new_size = 100\n",
    "img_res = resize_padding(img, new_size)\n",
    "axs[2].imshow(img_res.astype(int), vmin=0, vmax=255)\n",
    "axs[2].set_title(\"minimized image\")\n",
    "\n",
    "#zoomin \n",
    "a = img\n",
    "zommin = int(224*1.6)\n",
    "origin_sz = 224\n",
    "# cut = (zommin-origin_sz)//2\n",
    "# a1 = keras.layers.Resizing(zommin, zommin, interpolation='bilinear', crop_to_aspect_ratio=False,)(a).numpy()\n",
    "# a2 = a1[cut:zommin-cut,cut:zommin-cut]\n",
    "\n",
    "a2 = zoomin_img(img,zommin)\n",
    "axs[3].imshow(a2.astype(int), vmin=0, vmax=255)\n",
    "axs[3].set_title(\"maximized image\")\n",
    "\n",
    "# print(a2.shape)\n",
    "# fig1, axs1 = plt.subplots(1,2, figsize=(12, 6))\n",
    "# axs1[0].imshow(img.astype(int), vmin=0, vmax=255)\n",
    "# axs1[1].imshow(a2.astype(int), vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abc0f5a",
   "metadata": {},
   "source": [
    "### ROIs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8eebda60",
   "metadata": {},
   "source": [
    "corr_dict_subjects = {key : corr_list_RDM(RDM_fmri_s1[key], RDM_fmri_s2[key]) for key in RDM_fmri_s1.keys()} \n",
    "corr_dict_subjects\n",
    "# display(RDM_fmri_s1['ROIs/floc-bodies']['EBA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb112c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Voxels Bold signal from voxels_vis_ROIS V1 ventral\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.set(xlabel=\"Voxel\", ylabel=\"Stimulus\")\n",
    "heatmap = ax.imshow(voxels_ROIs_full['prf-visualrois']['V1v'],\n",
    "                    aspect=\"auto\", vmin=-1, vmax=1, cmap=\"bwr\")\n",
    "fig.colorbar(heatmap, shrink=.5, label=\"Response amplitude (Z)\")\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ee2955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RDM for fmri data\n",
    "RDM_fmri = {}\n",
    "for area in voxels_ROIs_full.keys():\n",
    "    RDM_fmri[area] = {}\n",
    "    for roi in voxels_ROIs_full[area].keys():\n",
    "        RDM_fmri[area][roi] = 1-np.corrcoef(voxels_ROIs_full[area][roi][:net_lim,:])\n",
    "    print(\"Created RDM for: \" + str(area))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1098f49",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RDM_fmri' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32988/3127278322.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRDM_fmri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'RDM_fmri' is not defined"
     ]
    }
   ],
   "source": [
    "RDM_fmri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6d69cc",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f516a442",
   "metadata": {},
   "source": [
    "# Applying VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c07de6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pic_edit():\n",
    "    \"\"\"\n",
    "    This class should ease applying filters and edits to the cut squered pictue before they are being proccessed by the net. \n",
    "    It aims to contain all present and future potential dynamic properties of a picture.\n",
    "    \"\"\"\n",
    "    def __init__(self, size = 224, blur = 1):\n",
    "        self.size = size\n",
    "        self.blur = blur\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'pic({self.size, self.blur})'\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.size, self.blur))\n",
    "    \n",
    "d = {pic_edit(224,1) :1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020981b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulilding Net\n",
    "net = keras.applications.VGG16(input_shape=(net_size, net_size, 3),\n",
    "                                include_top=True, weights='imagenet')\n",
    "\n",
    "def preprocss_net(input_size, cut_ims, pic_params):\n",
    "    \"\"\"\n",
    "    This function exectute all needed and optional steps before implementing the network over the data\n",
    "    it resizes the input, and when needed it decreses and creates padding\n",
    "    \"\"\" \n",
    "    input_len = len(cut_ims)\n",
    "    input_net = np.zeros((input_len, input_size, input_size,3))\n",
    "    for i in range(input_len):       \n",
    "        input_net[i] = keras.layers.Resizing(input_size, input_size, interpolation='bilinear', crop_to_aspect_ratio=False,)(cut_ims[i]).numpy()\n",
    "        if pic_params.blur > 0:\n",
    "            input_net[i] = cv2.blur(input_net[i], (pic_params.blur, pic_params.blur))\n",
    "        if pic_params.size < input_size:\n",
    "            input_net[i] = resize_padding(input_net[i], pic_params.size)\n",
    "        elif pic_params.size > input_size:\n",
    "            cut = (pic_params.size-net_size)//2\n",
    "            zoomin = keras.layers.Resizing(pic_params.size, pic_params.size, interpolation='bilinear', crop_to_aspect_ratio=False,)(input_net[i]).numpy()\n",
    "            cutzoom = zoomin[cut:net_size+cut,cut:net_size+cut]\n",
    "            if i < 1:\n",
    "                plt.imshow(cutzoom.astype(int), vmin=0, vmax=255)\n",
    "                plt.show()\n",
    "            input_net[i] = cutzoom\n",
    "    input_net = input_net[:net_lim,...]\n",
    "    input_net = input_net.astype(int)\n",
    "    return keras.applications.vgg16.preprocess_input(input_net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5483b6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN applying\n",
    "def preds_from_batches(layer_to_eval,data,batch_size = 256, sample_size=-1 ):\n",
    "    \"\"\"\n",
    "    Sasha's function for CNN application. Processing each layer and saving the output\n",
    "    \"\"\"\n",
    "    data_len = data.shape[0]\n",
    "    samples_set_flag = False\n",
    "    for batch in range(data_len//batch_size):\n",
    "        start = batch * batch_size\n",
    "        end = min((batch + 1) * batch_size, data_len)\n",
    "        result = layer_to_eval(data[start:end]).numpy().reshape([end-start,-1])\n",
    "        if not samples_set_flag and sample_size > 0:\n",
    "            samples = np.random.choice(result.shape[-1], sample_size)\n",
    "            samples_set_flag = True\n",
    "        if batch == 0:\n",
    "            ll_out = np.zeros([data_len,result.shape[-1]]) if sample_size == -1 else np.zeros([data_len,sample_size])\n",
    "        ll_out[start:end,:] = result if sample_size == -1 else result[:,samples]\n",
    "    return ll_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2146a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_list_extractor(RDM_net, RDM_fmri, corr_fun=spearmanr):\n",
    "    \"\"\"\n",
    "    This function takes all RDMs from all network layers, and all RDMS from FMRI voxels and calculate correlations.\n",
    "    \"\"\"\n",
    "    corr_dict = {}\n",
    "    for brain_area in RDM_fmri.keys():\n",
    "        corr_dict[brain_area] = []\n",
    "        for cnn_layer in range(len(RDM_net)):\n",
    "            sz=RDM_net[cnn_layer].shape[0]\n",
    "            mask = np.triu(np.ones([sz,sz],dtype=np.bool),1)\n",
    "            corr, pval = corr_fun(np.nan_to_num(RDM_net[cnn_layer][mask]),RDM_fmri[brain_area][mask])\n",
    "            corr_dict[brain_area].append(corr)\n",
    "    print (\"done correlations \")\n",
    "    return corr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c9099e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pic_edit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32988/3491102784.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# RDM of network layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# params_list = [pic_edit(net_size,0), pic_edit(int(net_size*0.5),0), pic_edit(int(net_size*1.5),0), pic_edit(int(net_size*2),0),  pic_edit(net_size,5)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mparams_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpic_edit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mRDM_net_sz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pic_edit' is not defined"
     ]
    }
   ],
   "source": [
    "# RDM of network layers\n",
    "# params_list = [pic_edit(net_size,0), pic_edit(int(net_size*0.5),0), pic_edit(int(net_size*1.5),0), pic_edit(int(net_size*2),0),  pic_edit(net_size,5)]\n",
    "params_list = [pic_edit(net_size,0)]\n",
    "\n",
    "RDM_net_sz={}\n",
    "for params in params_list:\n",
    "    RDM_net_sz[params] = {}\n",
    "    print(\"applying network for images with params: \\r\\n image size:\" + str(params.size) + \" || blur filter size: \"+ str(params.blur))\n",
    "    input_net = preprocss_net(net_size, cutims, params)\n",
    "    print(input_net.shape)\n",
    "    for ll, layer in enumerate(net.layers):\n",
    "            print(ll, layer.name)\n",
    "            feature_extractor = keras.Model(inputs=net.inputs,outputs=layer.output,)\n",
    "            activity = preds_from_batches(feature_extractor, input_net,batch_size=batch_size, sample_size=3000)\n",
    "            RDM_net_sz[params][ll] = 1-np.corrcoef(activity)\n",
    "            del feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3898955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutims.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4369d788",
   "metadata": {},
   "source": [
    "this is the level of the execution of the network. we can run the network as many times as we wish, changing the parameters of the pictures as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c836b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDM_net_sz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a92bf9e",
   "metadata": {},
   "source": [
    "# plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd3794de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RDM_net_sz' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32988/4071660142.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorr_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0msz\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mcorr_list_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRDM_net_sz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDM_fmri\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mRDM_fmri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mRDM_net_sz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'RDM_net_sz' is not defined"
     ]
    }
   ],
   "source": [
    "corr_dict = {sz : {key : corr_list_extractor(RDM_net_sz[sz], RDM_fmri[key]) for key in RDM_fmri.keys()} for sz in RDM_net_sz.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6668f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_dict_ecog = {sz : corr_list_extractor(RDM_net_sz[sz], RDMv) for sz in RDM_net_sz.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d589c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_dict_ecog"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f69f130a",
   "metadata": {},
   "source": [
    "# calculate correlation between RDMs and plot for each ROI\n",
    "for area in corr_dict[list(corr_dict.keys())[0]].keys():\n",
    "    fig, axs = plt.subplots(1,len(list(corr_dict.keys())), figsize=(16, 8),  sharey=True)\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        [ax.plot(corr_dict[list(corr_dict.keys())[i]][area][roi], '*-') for roi in list(corr_dict[list(corr_dict.keys())[i]][area].keys())]\n",
    "        ax.set_title( \"imsize: \" + str(list(corr_dict.keys())[i].size) + \"|blur: \" + str(list(corr_dict.keys())[i].blur))\n",
    "        #     [ax2.plot(corr_dict[224][area][roi], '*-') for roi in corr_dict[224][area].keys()]\n",
    "    fig.legend(corr_dict[list(corr_dict.keys())[0]][area].keys())\n",
    "    fig.suptitle(\"correlation for area: \" + area, size=18)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "798d1363",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corr_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32988/1534830843.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorr_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'corr_dict' is not defined"
     ]
    }
   ],
   "source": [
    "corr_dict[list(corr_dict.keys())[0]].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5dd5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [axs[0].get_lines()[i].get_color() for i in range(len(list(axs[0].get_lines())))]\n",
    "colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255e20d",
   "metadata": {},
   "source": [
    "# bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed16fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(mat,mat_):\n",
    "    ii = np.random.randint(mat.shape[0],size=(mat.shape[0]))\n",
    "    not_diag = np.eye(*mat.shape)<1e-5\n",
    "    return mat[:,ii][ii,:],mat_[:,ii][ii,:],not_diag[:,ii][ii,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520fbcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_list_extractor_resampled(RDM_net,RDM_fmri,corr_fun=spearmanr):\n",
    "    \"\"\"\n",
    "    fOR BOOTSTRAP This function takes all RDMs from all network layers, and all RDMS from FMRI voxels and calculate correlations.\n",
    "    \"\"\"\n",
    "    corr_dict = {}\n",
    "    for brain_area in RDM_fmri.keys():\n",
    "        corr_dict[brain_area] = []\n",
    "        for cnn_layer in range(len(RDM_net)):\n",
    "            sz=RDM_net[cnn_layer].shape[0]\n",
    "            mask = np.triu(np.ones([sz,sz],dtype=np.bool),1)\n",
    "            RDM1, RDM2, resample_mask = resample(RDM_net[cnn_layer],RDM_fmri[brain_area])\n",
    "            mask = np.logical_and(mask, resample_mask)\n",
    "            corr, pval = corr_fun(np.nan_to_num(RDM1[mask]),\n",
    "                                  RDM2[mask])\n",
    "            corr_dict[brain_area].append(corr)\n",
    "        corr_dict[brain_area] = np.array(corr_dict[brain_area])#.reshape([2,-1])\n",
    "    print (\"done correlations \")\n",
    "    return corr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834e1b0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_resamples = 10\n",
    "corr_dict_vis = {samp: {sz : corr_list_extractor_resampled(RDM_net_sz[sz], RDMv)  for sz in RDM_net_sz.keys()} for samp in range(n_resamples)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a6e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_resamples = 10\n",
    "corr_dict_bodies = {samp: {sz : corr_list_extractor_resampled(RDM_net_sz[sz], RDM_fmri['floc-bodies'])  for sz in RDM_net_sz.keys()} for samp in range(n_resamples)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c466a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_dict_vis_all= {}\n",
    "\n",
    "\n",
    "for area in corr_dict_vis[0][list(corr_dict_vis[0].keys())[0]].keys():\n",
    "    corr_dict_vis_all[area] = []\n",
    "    for ii in range(n_resamples):\n",
    "#         print(corr_dict_vis[ii][list(corr_dict_vis[0].keys())[0]][amp])\n",
    "        corr_dict_vis_all[area].append(corr_dict_vis[ii][list(corr_dict_vis[0].keys())[0]][area])\n",
    "    corr_dict_vis_all[area] = np.stack(corr_dict_vis_all[area])\n",
    "# corr_dict_vis_all['V1v'].shape\n",
    "# corr_dict_vis_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8061456",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corr_dict_bodies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32988/3326803590.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0marea\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorr_dict_bodies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr_dict_bodies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mcorr_dict_bodies_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marea\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_resamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corr_dict_bodies' is not defined"
     ]
    }
   ],
   "source": [
    "corr_dict_bodies_all= {}\n",
    "\n",
    "\n",
    "for area in corr_dict_bodies[0][list(corr_dict_bodies[0].keys())[0]].keys():\n",
    "    corr_dict_bodies_all[area] = []\n",
    "    for ii in range(n_resamples):\n",
    "#         print(corr_dict_vis[ii][list(corr_dict_vis[0].keys())[0]][amp])\n",
    "        corr_dict_bodies_all[area].append(corr_dict_bodies[ii][list(corr_dict_bodies[0].keys())[0]][area])\n",
    "    corr_dict_bodies_all[area] = np.stack(corr_dict_bodies_all[area])\n",
    "# corr_dict_vis_all\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3e2611a",
   "metadata": {},
   "source": [
    "# ll=0\n",
    "for i, key in enumerate(corr_dict_vis_all.keys()):\n",
    "    x_vec = [uu for uu in range(corr_dict_vis_all[key].shape[-1]) ]\n",
    "    plt.figure()\n",
    "    plt.errorbar(x_vec, corr_dict_vis_all[key].mean(axis=0),\n",
    "                 yerr=corr_dict_vis_all[key].std(axis=0), color = colors[i])\n",
    "    plt.title(key)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2dd3a3f1",
   "metadata": {},
   "source": [
    "for i, key in enumerate(corr_dict_vis_all.keys()):\n",
    "    x_vec = [uu for uu in range(corr_dict_vis_all['V1v'].shape[-1])]\n",
    "    plt.figure()\n",
    "    # plt.errorbar(corr_mat70_all.mean(axis=0).T,corr_mat70_all.std(axis=0).T)\n",
    "    # plt.plot(corr_mat70_all.mean(axis=0).T)\n",
    "    # plt.plot(corr_mat70_all.std(axis=0).T)\n",
    "    plt.plot(x_vec,\n",
    "                 corr_dict_vis_all[key].mean(axis=0), color = colors[i])\n",
    "    plt.fill_between(x_vec,\n",
    "                 corr_dict_vis_all[key].mean(axis=0)-corr_dict_vis_all[key].std(axis=0),\n",
    "                    corr_dict_vis_all[key].mean(axis=0)+corr_dict_vis_all[key].std(axis=0),alpha=0.4, color = colors[i])\n",
    "    plt.title(key)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9fb8bab5",
   "metadata": {},
   "source": [
    "for i, key in enumerate(corr_dict_bodies_all.keys()):\n",
    "    x_vec = [uu for uu in range(corr_dict_bodies_all['EBA'].shape[-1])]\n",
    "    plt.figure()\n",
    "    # plt.errorbar(corr_mat70_all.mean(axis=0).T,corr_mat70_all.std(axis=0).T)\n",
    "    # plt.plot(corr_mat70_all.mean(axis=0).T)\n",
    "    # plt.plot(corr_mat70_all.std(axis=0).T)\n",
    "    plt.plot(x_vec,\n",
    "                 corr_dict_bodies_all[key].mean(axis=0), color = colors[i])\n",
    "    plt.fill_between(x_vec,\n",
    "                 corr_dict_bodies_all[key].mean(axis=0)-corr_dict_bodies_all[key].std(axis=0),\n",
    "                    corr_dict_bodies_all[key].mean(axis=0)+corr_dict_bodies_all[key].std(axis=0),alpha=0.4, color = colors[i])\n",
    "    plt.title(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d47caf",
   "metadata": {},
   "source": [
    "# Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b3dc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "run_time\n",
    "\n",
    "path_for_save = \"/home/mor/NDS_project/results_data/sub\" + str(sub) + \"/results_\" + run_time\n",
    "path_for_save = '/home/mor/ECoG/subjects_data/results/FF_' + run_time\n",
    "\n",
    "# Check whether the specified path exists or not\n",
    "if not os.path.exists(path_for_save):\n",
    "   # Create a new directory because it does not exist\n",
    "   os.makedirs(path_for_save)\n",
    "   print(\"The new directory \"+ path_for_save +\" is created!\")\n",
    "\n",
    "# with open(path_for_save + '/corr_dict.pkl', 'wb') as handle:\n",
    "#     pkl.dump(corr_dict, handle, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "#     print(\"saved corr_dict!\")\n",
    "    \n",
    "with open(path_for_save + '/RDM_net_sz.pkl', 'wb') as handle:\n",
    "    pkl.dump(RDM_net_sz, handle, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "    print(\"saved RDM_net_sz!\")\n",
    "\n",
    "\n",
    "with open(path_for_save + '/corr_dict_vis_all.pkl', 'wb') as handle:\n",
    "    pkl.dump(corr_dict_vis_all, handle, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "    print(\"saved corr_dict_vis_all!\")\n",
    "        \n",
    "with open(path_for_save + '/results_explained.txt', 'a') as f:\n",
    "    f.write(\"Subject number: \" + str(sub))\n",
    "    f.write(input() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c9964",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_for_save + '/corr_dict_bodies_all.pkl', 'wb') as handle:\n",
    "    pkl.dump(corr_dict_bodies_all, handle, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "    print(\"saved corr_dict_bodies_all!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf573d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"/home/mor/NDS_project/results_data/sub3/results_202304020811/RDM_net_sz.pkl\", 'rb') as f:\n",
    "    RDM_net_sz = pkl.load(f)\n",
    "zer = RDM_net_sz[list(RDM_net_sz.keys())[0]]\n",
    "zer[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a84d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_dict_vis_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ca15f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('/home/mor/NDS_project/imported_datavoxels_ROIs.pkl', 'rb') as f:\n",
    "    voxels_ROIs_full1 = pkl.load(f)\n",
    "voxels_ROIs_full1['floc-bodies']['EBA'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10fddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + '/voxels_ROIs.pkl', 'rb') as f:\n",
    "    voxels_ROIs_s1_original = pkl.load(f)\n",
    "voxels_ROIs_s1_original.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc394a",
   "metadata": {},
   "source": [
    "### Data categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281707cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loding image metadata to find categories\n",
    "cat_path = \"/home/mor/annotations_trainval2017/annotations/\"\n",
    "fname = \"instances_train2017\"\n",
    "f = open(cat_path + fname +'.json', \"r\")\n",
    "data = json.loads(f.read())\n",
    "\n",
    "imid = data['annotations'][0]['image_id']\n",
    "imcat = data['annotations'][0]['category_id']\n",
    "data.keys()\n",
    "print([cat['name']  for cat in data['categories'] if cat['id'] == imcat])\n",
    "# catname = [cat['name'] if cat['id'] == imcat for cat in data['categories']]\n",
    "# data['categories'][0]\n",
    "imid\n",
    "\n",
    "im1= plt.imread(path+\"/train2017/train2017/000000\" + str(imid) +\".jpg\")\n",
    "plt.imshow(im1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
